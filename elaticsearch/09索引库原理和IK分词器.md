
## 09 索引库原理和IK分词器

ES 是基于Lunence创建的

### 01 ES底层存储位置

ES中的数据主要存储在ES文件下的 `\data`目录下

ES没有正常关闭，data中的数据不完整，启动的时候就有可能导致ES无法正常启动
解决措施： data文件夹删除掉，然后重新启动

数据丢失不可怕，利用闲暇时间重新生成索引数据即可

data里面的目录

\node       代表节点
\0          0目录 代表第一个节点
\indices    里面内容 节点唯一标识

基础数据向ES存储的过程中的作用是 创建索引


ES data(索引库) --> 索引区 + 元数据区(metadata)
 
索引区记录的是文章分词之后的效果
元数据区 存放的是一条一条完整的文档，同时会给文档一个唯一的标识 _id


### 02 分词器 Analyzer

1. 定义：就是将一段文本中的关键词汇拆分出来

    分词特点：拆分关键词，去掉停用词和语气词

2. ES中提供的分词器

    1. 默认 标准分词器 standard analyzer  英文 单词 中文 单字
    2. 简单 simple ayalyzer 英文 单词 去掉数字 中文 不分词

3. 测试不同分词器

```
GET _analyze
{
    "analyzer":"simple",
    "text":"redis very 好用"

}

```
4. github 基于 ES 每个版本写了一个分析器 -- IK分词器

版本严格与ES保持一致，

https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v7.7.1/elasticsearch-analysis-ik-7.7.1.zip


1. 本地安装 上传压缩包到linux中
2. 执行解压命令 `yum install -y unzip`  `uzip package_name`
3. 解压缩目录 直接放进 plugins 目录中

两种拆分方式
+ ik_max_word
+ ik_smart

ik分词器的两种使用方式

```
PUT /dangdang1
{
  "mappings":{
      "properties":{
        "id":{
          "type":"keyword",
          "analyzer": "ik_max_word"
        },
        "name":{"type":"keyword"},
        "price":{"type":"double"},
        "counts":{"type":"integer"}
      }
    }
}
```

```
GET _analyze
{
    "analyzer":"ik_max_word",
    "text":"我的中国梦，是2020年实现中华民族的伟大复兴"，
}
```

NSRIVEQFP001111ITCH-ID       IW125401024001234567890123456789012345            TLJ TN01AASEAW01QIA05100                  QANIO FE    QS00US00000000DA2020-06-18ZT16:59:16AP00


